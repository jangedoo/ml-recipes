{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "- To classify news articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip and extract.\n",
    "The dataset consists of 2225 documents and 5 categories: business, entertainment, politics, sport, tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./bbc/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use load_files function which loads text files with categories as subfolder names. Our dataset already has articles organized into different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_files(DATA_DIR, encoding=\"utf-8\", decode_error=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many categories and how many articles per category are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tech': 401, 'sport': 511, 'business': 510, 'entertainment': 386, 'politics': 417}\n"
     ]
    }
   ],
   "source": [
    "# calculate count of each category\n",
    "labels, counts = np.unique(data.target, return_counts=True)\n",
    "# convert data.target_names to np array for fancy indexing\n",
    "labels_str = np.array(data.target_names)[labels]\n",
    "print(dict(zip(labels_str, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each category has different number of articles. However, it does not look too imbalanced and the model should be able to learn properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1668, 557)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Kidman wins photographer battle\\n\\nActress Nicole Kidman has won a restraining ord',\n",
       " b'Godzilla gets Hollywood fame star\\n\\nMovie monster Godzilla has received a star on',\n",
       " b'Scrum-half Williams rejoins Bath\\n\\nBath have signed their former scrum-half Andy ',\n",
       " b'Yahoo moves into desktop search\\n\\nInternet giant Yahoo has launched software to a',\n",
       " b'BAA support ahead of court battle\\n\\nUK airport operator BAA has reiterated its su',\n",
       " b'Candidate resigns over BNP link\\n\\nA prospective candidate for the UK Independence',\n",
       " b\"The memory driving Brown's mission\\n\\nThe memory Gordon Brown says keeps returning\",\n",
       " b'Hollywood ready for Oscars night\\n\\nHollywood is preparing for the biggest night i',\n",
       " b\"Musical treatment for Capra film\\n\\nThe classic film It's A Wonderful Life is to b\",\n",
       " b\"UK broadband gets speed injection\\n\\nBroadband's rapid rise continues apace as spe\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at first 80 chars of some samles\n",
    "list(t[:80] for t in X_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go further, lets quickly go through what are the common natural language processing pipeline.\n",
    "- Tokenize i.e. split the text into words\n",
    "- Convert the case of letters to either upper or lower\n",
    "- Remove stopwords. For e.g. \"the\", \"an\", \"with\"\n",
    "- Perform stemming or lemmatization to reduce inflected words to its stem. For e.g. transportation -> transport, transported -> transport\n",
    "- (maybe some others)\n",
    "- Vectorization (Count, Binary, TF-IDF)\n",
    "\n",
    "Many libraries already exist to perform all of the steps mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in textual format and we cannot use it as it is. We need to convert it to a numerical format. A very common method, among others, is to calculate TF-IDF matrix. TF stands for term frequency in which we calculate how many times a term/word appears in a document.\n",
    "IDF stands for inverse document frequency which measures how important a word is. In simple terms it gives more weight to rare words than common ones.\n",
    "Once we calculate both TF and IDF, we can simply multiply them together to obtain TF-IDF value.\n",
    "tfidf(t, d, D) = tf(t, d) * idf(t, D) \n",
    "where,\n",
    "- t is a term\n",
    "- d is a document\n",
    "- D is set of all documents\n",
    "\n",
    "For details about TF-IDF check\n",
    "- http://www.tfidf.com/\n",
    "- https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='ignore',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000, decode_error=\"ignore\")\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used TfidfVectorizer to calculate TF-IDF. When initializing the vectorizer, we passed stop_words as \"english\" which tells sklearn to discard commonly occurring words in English. Then we also specifed max_features to 1000. The vectorizer will build a vocabulary of top 1000 words (by frequency). This means that each text in our dataset will be converted to a vector of size 1000.\n",
    "Next, we call **fit** function to \"train\" the vectorizer and also convert the list of texts into TF-IDF matrix. \n",
    "We can also use another function called **fit_transform**, which is equivalent to:\n",
    "```\n",
    "vectorizer.fit(X_train)\n",
    "X_train_vectorized = vectorizer.transform(X_train)\n",
    "```\n",
    "**Important** We should use only the training data to fit the vectorizer, otherwise it is cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cls = MultinomialNB()\n",
    "# transform the list of text to tf-idf before passing it to the model\n",
    "cls.fit(vectorizer.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.958707360862\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95       123\n",
      "          1       0.99      0.94      0.96       100\n",
      "          2       0.92      0.96      0.94        95\n",
      "          3       0.97      1.00      0.98       115\n",
      "          4       0.97      0.94      0.96       124\n",
      "\n",
      "avg / total       0.96      0.96      0.96       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "y_pred = cls.predict(vectorizer.transform(X_test))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% accuracy! Not bad. Let's see if we can find a better model. We'll train several models using sklearn Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# start with the classic\n",
    "# with either pure counts or tfidf features\n",
    "sgd = Pipeline([\n",
    "        (\"count vectorizer\", CountVectorizer(stop_words=\"english\", max_features=3000)),\n",
    "        (\"sgd\", SGDClassifier(loss=\"modified_huber\"))\n",
    "    ])\n",
    "sgd_tfidf = Pipeline([\n",
    "        (\"tfidf_vectorizer\", TfidfVectorizer(stop_words=\"english\", max_features=3000)), \n",
    "        (\"sgd\", SGDClassifier(loss=\"modified_huber\"))\n",
    "    ])\n",
    "\n",
    "svc = Pipeline([\n",
    "        (\"count_vectorizer\", CountVectorizer(stop_words=\"english\", max_features=3000)), \n",
    "        (\"linear svc\", SVC(kernel=\"linear\"))\n",
    "    ])\n",
    "svc_tfidf = Pipeline([\n",
    "        (\"tfidf_vectorizer\", TfidfVectorizer(stop_words=\"english\", max_features=3000)), \n",
    "        (\"linear svc\", SVC(kernel=\"linear\"))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('svc_tfidf', 0.973026575899821), ('svc', 0.95623710562069142), ('sgd_tfidf', 0.95384189603985314), ('sgd', 0.93645074796385619)]\n"
     ]
    }
   ],
   "source": [
    "all_models = [\n",
    "    (\"sgd\", sgd),\n",
    "    (\"sgd_tfidf\", sgd_tfidf),\n",
    "    (\"svc\", svc),\n",
    "    (\"svc_tfidf\", svc_tfidf),\n",
    "    ]\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, X_train, y_train, cv=2).mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine with tf-idf features scored the highest accuracy of 97%. Lets train it and evaluate it in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.978456014363\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.94      0.97       141\n",
      "          1       0.98      1.00      0.99        96\n",
      "          2       0.96      0.99      0.98        99\n",
      "          3       0.97      1.00      0.99       114\n",
      "          4       0.98      0.97      0.98       107\n",
      "\n",
      "avg / total       0.98      0.98      0.98       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = svc_tfidf\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98% accuracy! Unlike before, we don't have to vectorize the documents manually before passing it to the model, since we have defined the vectorization process in the pipeline itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
